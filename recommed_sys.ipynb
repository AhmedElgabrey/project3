{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"A parameter name that contains\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import textstat\n",
    "import joblib\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i just feel really helpless and heavy hearted</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive enjoyed being able to slouch about relax a...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i gave up my internship with the dmrg and am f...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont know i feel so lost</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am a kindergarten teacher and i am thoroughl...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422741</th>\n",
       "      <td>i begun to feel distressed for you</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422742</th>\n",
       "      <td>i left feeling annoyed and angry thinking that...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422743</th>\n",
       "      <td>i were to ever get married i d have everything...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422744</th>\n",
       "      <td>i feel reluctant in applying there because i w...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422745</th>\n",
       "      <td>i just wanted to apologize to you because i fe...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422746 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0           i just feel really helpless and heavy hearted   fear\n",
       "1       ive enjoyed being able to slouch about relax a...    sad\n",
       "2       i gave up my internship with the dmrg and am f...   fear\n",
       "3                              i dont know i feel so lost    sad\n",
       "4       i am a kindergarten teacher and i am thoroughl...   fear\n",
       "...                                                   ...    ...\n",
       "422741                 i begun to feel distressed for you   fear\n",
       "422742  i left feeling annoyed and angry thinking that...  anger\n",
       "422743  i were to ever get married i d have everything...    joy\n",
       "422744  i feel reluctant in applying there because i w...   fear\n",
       "422745  i just wanted to apologize to you because i fe...  anger\n",
       "\n",
       "[422746 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_prepare_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dataset and rename columns.\n",
    "\n",
    "    Args:\n",
    "        path (str): File path to CSV dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Prepared dataframe with columns ['text', 'label'].\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    data.rename(columns={'sentence': 'text', 'emotion': 'label'}, inplace=True)\n",
    "    return data\n",
    "\n",
    "data = load_and_prepare_data(r\"D:\\Grad_Proj\\project\\combined_emotion.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 422746 entries, 0 to 422745\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    422746 non-null  object\n",
      " 1   label   422746 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_of_Chars</th>\n",
       "      <th>No_of_Words</th>\n",
       "      <th>No_of_Sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>422746.000000</td>\n",
       "      <td>422746.000000</td>\n",
       "      <td>422746.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>97.033980</td>\n",
       "      <td>19.220179</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>56.198156</td>\n",
       "      <td>11.057121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>128.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>830.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         No_of_Chars    No_of_Words  No_of_Sents\n",
       "count  422746.000000  422746.000000     422746.0\n",
       "mean       97.033980      19.220179          1.0\n",
       "std        56.198156      11.057121          0.0\n",
       "min         2.000000       1.000000          1.0\n",
       "25%        54.000000      11.000000          1.0\n",
       "50%        86.000000      17.000000          1.0\n",
       "75%       128.000000      25.000000          1.0\n",
       "max       830.000000     178.000000          1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['No_of_Chars'] = data['text'].apply(len)\n",
    "data['No_of_Words'] = data.apply(lambda row: nltk.word_tokenize(row['text']), axis= 1).apply(len)\n",
    "data['No_of_Sents'] = data.apply(lambda row: nltk.sent_tokenize(row['text']), axis= 1).apply(len)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df: pd.DataFrame, col: str) -> (pd.DataFrame, LabelEncoder):\n",
    "    \"\"\"\n",
    "    Encode categorical labels to integers.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        col (str): Column name of labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dataframe with encoded labels, fitted LabelEncoder.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    return df, le\n",
    "\n",
    "data, le = encode_labels(data, 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\\nplt.figure(figsize=(12,8))\\nfg = sns.countplot(x= data['label'], palette= cols_color)\\nfg.set_title('count plot of classes')\\nfg.set_xlabel('classes')\\nfg.set_ylabel('count of classes') \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= data['label'], palette= cols_color)\n",
    "fg.set_title('count plot of classes')\n",
    "fg.set_xlabel('classes')\n",
    "fg.set_ylabel('count of classes') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" plt.figure(figsize=(12,8))\\nfg = sns.pairplot(data= data, hue= 'label', palette= cols_color)\\nplt.show(fg) \""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" plt.figure(figsize=(12,8))\n",
    "fg = sns.pairplot(data= data, hue= 'label', palette= cols_color)\n",
    "plt.show(fg) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    تحويل POS tag من شكل NLTK إلى الشكل المطلوب من WordNetLemmatizer.\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # الافتراضي\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    تنظيف النص مع تحسين lemmatization باستخدام POS tagging.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "        for word, tag in pos_tags\n",
    "        if word not in stop_words\n",
    "    ]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "data['lemmatized_words'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\\nplt.figure(figsize=(12,8))\\nfg = sns.countplot(x= data['label'], palette= cols_color)\\nfg.set_title('count plot of classes')\\nfg.set_xlabel('classes')\\nfg.set_ylabel('count of classes') \""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= data['label'], palette= cols_color)\n",
    "fg.set_title('count plot of classes')\n",
    "fg.set_xlabel('classes')\n",
    "fg.set_ylabel('count of classes') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' all_words = \" \".join(sentence for sentence in data[\\'lemmatized_words\\'])\\nall_words\\n\\nwordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\\n\\nplt.figure(figsize=(12,8))\\nplt.imshow(wordcloud, interpolation=\\'bilinear\\')\\nplt.axis(\\'off\\')\\nplt.show() '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" all_words = \" \".join(sentence for sentence in data['lemmatized_words'])\n",
    "all_words\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000, max_df=0.9, min_df=2, stop_words='english', ngram_range=(1, 2))\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vectorize_text(df: pd.DataFrame, text_col: str, vectorizer: TfidfVectorizer):\n",
    "    \"\"\"\n",
    "    Vectorize text column using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe containing text data.\n",
    "        text_col (str): Name of the column with preprocessed text.\n",
    "        vectorizer (TfidfVectorizer): Initialized vectorizer.\n",
    "\n",
    "    Returns:\n",
    "        sparse matrix: TF-IDF features matrix.\n",
    "    \"\"\"\n",
    "    X = vectorizer.fit_transform(df[text_col])\n",
    "    return X\n",
    "\n",
    "sampled_data = data.sample(frac=0.3, random_state=42)\n",
    "X = vectorize_text(data, 'lemmatized_words', tfidf)\n",
    "y = data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def extract_additional_features(text: str) -> list:\\n    words = word_tokenize(text)\\n    num_words = len(words)\\n    num_chars = len(text)\\n    sentiment = sia.polarity_scores(text)['compound']\\n    readability = textstat.flesch_reading_ease(text)\\n    return [num_words, num_chars, sentiment, readability]\\n\\nadditional_features = np.array([extract_additional_features(text) for text in data['lemmatized_words']])\\nscaler = StandardScaler()\\nadditional_features_scaled = scaler.fit_transform(additional_features) \""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def extract_additional_features(text: str) -> list:\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    num_chars = len(text)\n",
    "    sentiment = sia.polarity_scores(text)['compound']\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    return [num_words, num_chars, sentiment, readability]\n",
    "\n",
    "additional_features = np.array([extract_additional_features(text) for text in data['lemmatized_words']])\n",
    "scaler = StandardScaler()\n",
    "additional_features_scaled = scaler.fit_transform(additional_features) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from scipy.sparse import hstack\\nfrom scipy.sparse import csr_matrix\\n\\nX_combined = hstack([X, csr_matrix(additional_features_scaled)])\\n '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_combined = hstack([X, csr_matrix(additional_features_scaled)])\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           NN RB JJ NN VBD\n",
       "1         JJ NN JJ JJ NN IN JJ NN JJ NN RB VBP JJ NN RB ...\n",
       "2                                            VB NN NN NN NN\n",
       "3                                              NN VBP NN VB\n",
       "4         VB PRP RB JJ NN VB NN NN NN NN NN NN VBP NN NN...\n",
       "                                ...                        \n",
       "422741                                            VB NNS JJ\n",
       "422742                              VB NN IN JJ NN NN JJ NN\n",
       "422743    RB VB JJ NN NN VBP VB RB VBP RB RB VBP JJ NN N...\n",
       "422744                 NN JJ NN VBP JJ VBP NN VBP JJS CD NN\n",
       "422745                                    JJ NN NN IN NN NN\n",
       "Name: pos_tags, Length: 422746, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_pos_tags(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return ' '.join(tag for word, tag in pos_tags)\n",
    "\n",
    "data['pos_tags'] = data['lemmatized_words'].apply(extract_pos_tags)\n",
    "\n",
    "tfidf_pos = TfidfVectorizer()\n",
    "X_pos = tfidf_pos.fit_transform(data['pos_tags'])\n",
    "data['pos_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: [ 47454  39719 114453  27643  96949  11978]\n",
      "After SMOTE: [114453 114453 114453 114453 114453 114453]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", np.bincount(y_train))\n",
    "print(\"After SMOTE:\", np.bincount(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({2: 114453, 4: 96949, 0: 47454, 1: 39719, 3: 27643, 5: 11978})\n",
      "After SMOTE: Counter({4: 114453, 1: 114453, 2: 114453, 0: 114453, 3: 114453, 5: 114453})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "print(\"After SMOTE:\", Counter(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 262: 0.1263050337217284\n",
      "index 350: 0.5806697162326915\n",
      "index 478: 0.6752847771992059\n",
      "index 739: 0.4368754843085354\n"
     ]
    }
   ],
   "source": [
    "row = X[0].toarray().flatten()\n",
    "for i, val in enumerate(row):\n",
    "    if val != 0:\n",
    "        print(f'index {i}: {val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7645\n",
      "Test set F1 Score (weighted): 0.7672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75     11863\n",
      "           1       0.71      0.71      0.71      9930\n",
      "           2       0.82      0.79      0.80     28614\n",
      "           3       0.57      0.65      0.61      6911\n",
      "           4       0.86      0.80      0.83     24238\n",
      "           5       0.46      0.55      0.50      2994\n",
      "\n",
      "    accuracy                           0.76     84550\n",
      "   macro avg       0.69      0.72      0.70     84550\n",
      "weighted avg       0.77      0.76      0.77     84550\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.72      0.78      0.75     11863\n",
      "        fear       0.71      0.71      0.71      9930\n",
      "         joy       0.82      0.79      0.80     28614\n",
      "        love       0.57      0.65      0.61      6911\n",
      "         sad       0.86      0.80      0.83     24238\n",
      "     suprise       0.46      0.55      0.50      2994\n",
      "\n",
      "    accuracy                           0.76     84550\n",
      "   macro avg       0.69      0.72      0.70     84550\n",
      "weighted avg       0.77      0.76      0.77     84550\n",
      "\n",
      "F1 Score: 0.7672384289333223\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train Random Forest and evaluate on test set.\n",
    "\n",
    "    Args:\n",
    "        X_train, y_train: Training features and labels.\n",
    "        X_test, y_test: Testing features and labels.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained Random Forest model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \"\"\"  param_grid = {\n",
    "    'n_estimators': [100, 200, 300],           # عدد الأشجار\n",
    "    'max_depth': [None, 10, 20, 30],           # أقصى عمق للشجرة\n",
    "    'min_samples_split': [2, 5, 10]             # أقل عدد عينات لتقسيم عقدة\n",
    "    } \"\"\"\n",
    "    \"\"\"  \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,cv=3, n_jobs=-1, verbose=2, scoring='f1_weighted')\n",
    "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    print(\"أفضل المعاملات:\", grid_search.best_params_)\n",
    "    print(\"أفضل نتيجة F1 (weighted):\", grid_search.best_score_)\n",
    "    \n",
    "    best_rf_model = grid_search.best_estimator_ \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Test set F1 Score (weighted): {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    \"\"\" cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show() \"\"\"\n",
    "    \n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    return model\n",
    "\n",
    "random_forest_model = train_and_evaluate(X_train_balanced, y_train_balanced, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison between models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' models = {\\n    \\'Random Forest\\': RandomForestClassifier(),\\n    \\'Naive Bayes\\': MultinomialNB(),\\n    \\'SVM\\': SVC(),\\n    \\'KNN\\': KNeighborsClassifier()\\n}\\n\\nfor name, model in models.items():\\n    model.fit(X_train_balanced, y_train_balanced)\\n    y_pred = model.predict(X_test)\\n    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\\n    print(f\"{name} F1 Score: {f1_score(y_test, y_pred, average=\\'weighted\\'):.2f}\")\\n    print(\"-\" * 30)\\n '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"{name} F1 Score: {f1_score(y_test, y_pred, average='weighted'):.2f}\")\n",
    "    print(\"-\" * 30)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/label_encoder.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(random_forest_model, './models/random_forest_model.pkl')\n",
    "joblib.dump(tfidf, './models/tfidf_vectorizer.pkl')\n",
    "joblib.dump(le, './models/label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(text, model, vectorizer, label_encoder):\n",
    "    processed = preprocess_text(text)\n",
    "    \n",
    "    vect_text = vectorizer.transform([processed])\n",
    "    \n",
    "    prediction = model.predict(vect_text)\n",
    "    return label_encoder.inverse_transform(prediction)[0]\n",
    "\n",
    "print(predict_emotion(\"I feel so happy and joyful today!\", random_forest_model, tfidf, le))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({2: 114453, 4: 96949, 0: 47454, 1: 39719, 3: 27643, 5: 11978})\n",
      "After SMOTE: Counter({4: 114453, 1: 114453, 2: 114453, 0: 114453, 3: 114453, 5: 114453})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "print(\"After SMOTE:\", Counter(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2767052</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41865</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F. Scott Fitzgerald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130616</th>\n",
       "      <td>Bayou Moon</td>\n",
       "      <td>Ilona Andrews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208324</th>\n",
       "      <td>Means of Ascent</td>\n",
       "      <td>Robert A. Caro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77431</th>\n",
       "      <td>The Mauritius Command</td>\n",
       "      <td>Patrick O'Brian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565083</th>\n",
       "      <td>Cinderella Ate My Daughter: Dispatches from th...</td>\n",
       "      <td>Peggy Orenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8914</th>\n",
       "      <td>The First World War</td>\n",
       "      <td>John Keegan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  \\\n",
       "book_id                                                      \n",
       "2767052                                   The Hunger Games   \n",
       "3                 Harry Potter and the Philosopher's Stone   \n",
       "41865                                             Twilight   \n",
       "2657                                 To Kill a Mockingbird   \n",
       "4671                                      The Great Gatsby   \n",
       "...                                                    ...   \n",
       "7130616                                         Bayou Moon   \n",
       "208324                                    Means of Ascent    \n",
       "77431                                The Mauritius Command   \n",
       "8565083  Cinderella Ate My Daughter: Dispatches from th...   \n",
       "8914                                   The First World War   \n",
       "\n",
       "                             authors  \n",
       "book_id                               \n",
       "2767052              Suzanne Collins  \n",
       "3        J.K. Rowling, Mary GrandPré  \n",
       "41865                Stephenie Meyer  \n",
       "2657                      Harper Lee  \n",
       "4671             F. Scott Fitzgerald  \n",
       "...                              ...  \n",
       "7130616                Ilona Andrews  \n",
       "208324                Robert A. Caro  \n",
       "77431                Patrick O'Brian  \n",
       "8565083              Peggy Orenstein  \n",
       "8914                     John Keegan  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_db = pd.read_csv(\"books.csv\", index_col='book_id')\n",
    "books_db = books_db[['original_title', 'authors']]\n",
    "books_db.rename(columns={'original_title': 'title'}, inplace=True)\n",
    "books_db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 2767052 to 8914\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    9415 non-null   object\n",
      " 1   authors  10000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "books_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_db.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://towardsdatascience.com/not-all-rainbow...</td>\n",
       "      <td>Not All Rainbows and Sunshine: The Darker Side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://towardsdatascience.com/ethics-in-ai-po...</td>\n",
       "      <td>Ethics in AI: Potential Root Causes for Biased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://towardsdatascience.com/python-tuple-th...</td>\n",
       "      <td>Python Tuple, The Whole Truth and Only the Tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://towardsdatascience.com/dates-and-subqu...</td>\n",
       "      <td>Dates and Subqueries in SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://towardsdatascience.com/temporal-differ...</td>\n",
       "      <td>Temporal Differences with Python: First Sample...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>https://medium.com/swlh/brian-chesky-is-an-exa...</td>\n",
       "      <td>Brian Chesky is an Example of What it Means to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>https://medium.com/swlh/5-red-flags-of-online-...</td>\n",
       "      <td>5 Red Flags of Online Business Gurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>https://writingcooperative.com/recognizing-the...</td>\n",
       "      <td>Recognizing These Three Realities Can Help Set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>https://writingcooperative.com/i-remember-it-l...</td>\n",
       "      <td>“I Remember It Like It Was Just Yesterday…” Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>https://writingcooperative.com/how-to-formulat...</td>\n",
       "      <td>How to Formulate a Great Nonfiction Theme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2498 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "id                                                        \n",
       "1     https://towardsdatascience.com/not-all-rainbow...   \n",
       "2     https://towardsdatascience.com/ethics-in-ai-po...   \n",
       "3     https://towardsdatascience.com/python-tuple-th...   \n",
       "4     https://towardsdatascience.com/dates-and-subqu...   \n",
       "5     https://towardsdatascience.com/temporal-differ...   \n",
       "...                                                 ...   \n",
       "2494  https://medium.com/swlh/brian-chesky-is-an-exa...   \n",
       "2495  https://medium.com/swlh/5-red-flags-of-online-...   \n",
       "2496  https://writingcooperative.com/recognizing-the...   \n",
       "2497  https://writingcooperative.com/i-remember-it-l...   \n",
       "2498  https://writingcooperative.com/how-to-formulat...   \n",
       "\n",
       "                                                  title  \n",
       "id                                                       \n",
       "1     Not All Rainbows and Sunshine: The Darker Side...  \n",
       "2     Ethics in AI: Potential Root Causes for Biased...  \n",
       "3     Python Tuple, The Whole Truth and Only the Tru...  \n",
       "4                           Dates and Subqueries in SQL  \n",
       "5     Temporal Differences with Python: First Sample...  \n",
       "...                                                 ...  \n",
       "2494  Brian Chesky is an Example of What it Means to...  \n",
       "2495               5 Red Flags of Online Business Gurus  \n",
       "2496  Recognizing These Three Realities Can Help Set...  \n",
       "2497  “I Remember It Like It Was Just Yesterday…” Re...  \n",
       "2498          How to Formulate a Great Nonfiction Theme  \n",
       "\n",
       "[2498 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_db = pd.read_csv(\"articles.csv\", index_col='id')  # يحتوي على title, link\n",
    "articles_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2498 entries, 1 to 2498\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     2498 non-null   object\n",
      " 1   title   2498 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 58.5+ KB\n"
     ]
    }
   ],
   "source": [
    "articles_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"./models/random_forest_model.pkl\")\n",
    "vectorizer = joblib.load(\"./models/tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "def classify_emotions(\n",
    "    df: pd.DataFrame,\n",
    "    text_column: str,\n",
    "    model: BaseEstimator,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    le: LabelEncoder,\n",
    "    positive_emotions: List[str] = [\"joy\", \"love\", \"surprise\"],\n",
    "    save_path: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    يصنف المشاعر في عمود نصي ويُرجع فقط الصفوف ذات المشاعر الإيجابية.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): داتا فريم يحتوي على النصوص.\n",
    "        text_column (str): اسم العمود الذي يحتوي على النصوص (مثل: \"title\" أو \"book_title\").\n",
    "        model (BaseEstimator): نموذج تصنيف مدرب.\n",
    "        vectorizer (TfidfVectorizer): محول TF-IDF.\n",
    "        scaler (StandardScaler): مقياس الميزات الإضافية.\n",
    "        le (LabelEncoder): مشفر العواطف.\n",
    "        positive_emotions (List[str], optional): قائمة بالعواطف الإيجابية للاحتفاظ بها.\n",
    "        save_path (str, optional): إذا تم توفيره، سيتم حفظ الداتا فريم النهائي في هذا المسار كملف CSV.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: داتا فريم يحتوي على النصوص المصنفة ذات العواطف الإيجابية.\n",
    "    \"\"\"\n",
    "\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"❌ العمود '{text_column}' غير موجود في الداتا فريم.\")\n",
    "\n",
    "    df['clean_text'] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "    X_text = vectorizer.transform(df['clean_text'])\n",
    "\n",
    "\n",
    "    predicted = model.predict(X_text)\n",
    "    df['emotion'] = le.inverse_transform(predicted)\n",
    "\n",
    "    df_filtered = df[df['emotion'].isin(positive_emotions)]\n",
    "\n",
    "\n",
    "    if save_path:\n",
    "        df_filtered.to_csv(save_path, index=False)\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_clean = classify_emotions(\n",
    "        df=books_db,\n",
    "        text_column=\"title\",\n",
    "        model=model,\n",
    "        vectorizer=vectorizer,\n",
    "        le=le,\n",
    "        save_path=\"classified_books.csv\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_clean = classify_emotions(\n",
    "        df=articles_db,\n",
    "        text_column=\"title\",\n",
    "        model=model,\n",
    "        vectorizer=vectorizer,\n",
    "        le=le,\n",
    "        save_path=\"classified_articles.csv\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_keywords(df: pd.DataFrame, include_keywords=None, exclude_keywords=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ترشيح المحتوى بناءً على كلمات مفتاحية.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): جدول المحتوى.\n",
    "        include_keywords (list): كلمات يجب أن يحتويها النص.\n",
    "        exclude_keywords (list): كلمات يجب ألا يحتويها النص.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: محتوى بعد التصفية.\n",
    "    \"\"\"\n",
    "    if include_keywords:\n",
    "        pattern = '|'.join(include_keywords)\n",
    "        df = df[df['title'].str.contains(pattern, case=False, na=False)]\n",
    "    \n",
    "    if exclude_keywords:\n",
    "        pattern = '|'.join(exclude_keywords)\n",
    "        df = df[~df['title'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_map = {\n",
    "    'sadness': ['joy', 'surprise'],\n",
    "    'anger': ['love', 'joy'],\n",
    "    'fear': ['love', 'joy'],\n",
    "    'joy': ['joy','surprise', 'love'],\n",
    "    'surprise': ['love', 'joy'],\n",
    "    'love': ['joy', 'love', 'surprise']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recommend_content_filtered(\n",
    "    user_text: str,\n",
    "    model,\n",
    "    vectorizer,\n",
    "    label_encoder,\n",
    "    rec_db: pd.DataFrame,\n",
    "    include_keywords=None,\n",
    "    exclude_keywords=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    #توصية بمحتوى مع فلترة اختيارية بناءً على الكلمات المفتاحية.\n",
    "    \"\"\"\n",
    "    # تنظيف أسماء الأعمدة من المسافات الزائدة\n",
    "    rec_db.columns = rec_db.columns.str.strip()\n",
    "\n",
    "    # التأكد من وجود الأعمدة المطلوبة\n",
    "    if 'predicted_emotion' not in rec_db.columns or 'original_title' not in rec_db.columns:\n",
    "        raise KeyError(\"⚠️ تأكد أن الداتا تحتوي على الأعمدة: 'original_title' و 'predicted_emotion'.\")\n",
    "\n",
    "    # التنبؤ بالمشاعر للنص\n",
    "    user_emotion = predict_emotion(user_text, model, vectorizer, label_encoder)\n",
    "    print(f\"🔍 Detected Emotion: {user_emotion}\")\n",
    "\n",
    "    # تحديد المشاعر المستهدفة بناءً على mood_map\n",
    "    target_emotions = mood_map.get(user_emotion, ['joy', 'calm', 'confidence'])\n",
    "\n",
    "    # ترشيح المحتوى بناءً على المشاعر\n",
    "    recommended = rec_db[rec_db['predicted_emotion'].isin(target_emotions)]\n",
    "\n",
    "    # فلترة المحتوى بناءً على الكلمات\n",
    "    filtered = filter_by_keywords(recommended, include_keywords, exclude_keywords)\n",
    "\n",
    "    # في حالة عدم وجود نتائج بعد الفلترة\n",
    "    if filtered.empty:\n",
    "        print(\"⚠️ لا يوجد محتوى يلبي معايير الفلترة. عرض نتائج بدون فلترة.\")\n",
    "        return recommended[['original_title', 'authors','predicted_emotion']].sample(min(3, len(recommended)))\n",
    "\n",
    "    return filtered[['original_title', 'authors', 'predicted_emotion']].sample(min(3, len(filtered)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(emotion: str, top_n: int = 5) -> List[Dict]:\n",
    "    df = pd.read_csv(\"classified_books.csv\")  \n",
    "\n",
    "    if \"emotion\" not in df.columns:\n",
    "        df[\"emotion\"] = df[\"title\"].apply(predict_emotion)\n",
    "\n",
    "    target_emotions = mood_map.get(emotion, [\"joy\", \"love\", \"surprise\"])\n",
    "\n",
    "    recommended = df[df[\"emotion\"].isin(target_emotions)].head(top_n)\n",
    "\n",
    "    return recommended[[\"title\", \"authors\", \"emotion\"]].to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles(emotion: str, top_n: int = 5) -> List[Dict]:\n",
    "    df = pd.read_csv(\"classified_articles.csv\")\n",
    "\n",
    "    if \"emotion\" not in df.columns:\n",
    "        df[\"emotion\"] = df[\"title\"].apply(predict_emotion)\n",
    "\n",
    "    target_emotions = mood_map.get(emotion, [\"joy\", \"love\", \"surprise\"])\n",
    "\n",
    "    recommended = df[df[\"emotion\"].isin(target_emotions)].head(top_n)\n",
    "\n",
    "    return recommended[[\"title\", \"url\", \"emotion\"]].to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_content(emotion: str, top_n: int = 5) -> Dict[str, List[Dict]]:\n",
    "    books = recommend_books(emotion)[:top_n]\n",
    "    articles = recommend_articles(emotion)[:top_n]\n",
    "    return {\n",
    "        \"books\": books,\n",
    "        \"articles\": articles\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books:\n",
      "- Title: The Hunger Games, Authors: Suzanne Collins, Emotion: joy\n",
      "- Title: The Lovely Bones, Authors: Alice Sebold, Emotion: love\n",
      "- Title: Gone Girl, Authors: Gillian Flynn, Emotion: joy\n",
      "- Title: The Time Traveler's Wife, Authors: Audrey Niffenegger, Emotion: joy\n",
      "- Title: A Game of Thrones, Authors: George R.R. Martin, Emotion: joy\n",
      "\n",
      "Articles:\n",
      "- Title: Don’t Become a Full-Time Content Creator If You Have Low-Risk Tolerance, Link: None, Emotion: joy\n",
      "- Title: <strong class=\"markup--strong markup--h3-strong\">How My MBA Made Me a Better Fiction Writer</strong>, Link: None, Emotion: joy\n",
      "- Title: How to Start Your Novel with Momentum to Finish It, Link: None, Emotion: joy\n",
      "- Title: <strong class=\"markup--strong markup--h3-strong\">Using Propensity-Score Matching to Build Leading Indicators</strong>, Link: None, Emotion: joy\n",
      "- Title: Sparkles aren’t good UX✨, Link: None, Emotion: joy\n"
     ]
    }
   ],
   "source": [
    "results = recommend_content(emotion=\"sad\")\n",
    "\n",
    "print(\"Books:\")\n",
    "for item in results.get('books', []):\n",
    "    print(f\"- Title: {item.get('title')}, Authors: {item.get('authors')}, Emotion: {item.get('emotion')}\")\n",
    "\n",
    "print(\"\\nArticles:\")\n",
    "for item in results.get('articles', []):\n",
    "    print(f\"- Title: {item.get('title')}, Link: {item.get('link')}, Emotion: {item.get('emotion')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Hunger Games', 'authors': 'Suzanne Collins', 'emotion': 'joy'}\n",
      "{'title': 'The Lovely Bones', 'authors': 'Alice Sebold', 'emotion': 'love'}\n",
      "{'title': 'Gone Girl', 'authors': 'Gillian Flynn', 'emotion': 'joy'}\n",
      "{'title': \"The Time Traveler's Wife\", 'authors': 'Audrey Niffenegger', 'emotion': 'joy'}\n",
      "{'title': 'A Game of Thrones', 'authors': 'George R.R. Martin', 'emotion': 'joy'}\n"
     ]
    }
   ],
   "source": [
    "results = recommend_books(emotion=\"sad\")\n",
    "for book in results:\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Don’t Become a Full-Time Content Creator If You Have Low-Risk Tolerance', 'url': 'https://medium.com/swlh/dont-become-a-full-time-content-creator-if-you-have-low-risk-tolerance-13fa2f77791a', 'emotion': 'joy'}\n",
      "{'title': '<strong class=\"markup--strong markup--h3-strong\">How My MBA Made Me a Better Fiction\\xa0Writer</strong>', 'url': 'https://writingcooperative.com/how-my-mba-made-me-a-better-fiction-writer-d222bc61a5b0', 'emotion': 'joy'}\n",
      "{'title': 'How to Start Your Novel with Momentum to Finish\\xa0It', 'url': 'https://writingcooperative.com/how-to-start-your-novel-with-momentum-to-finish-it-8d001f4908c5', 'emotion': 'joy'}\n",
      "{'title': '<strong class=\"markup--strong markup--h3-strong\">Using Propensity-Score Matching to Build Leading Indicators</strong>', 'url': 'https://towardsdatascience.com/using-propensity-score-matching-to-build-leading-indicators-3e656dccbaf9', 'emotion': 'joy'}\n",
      "{'title': 'Sparkles aren’t good\\xa0UX✨', 'url': 'https://uxdesign.cc/sparkles-arent-good-ux-4b8199497c68', 'emotion': 'joy'}\n"
     ]
    }
   ],
   "source": [
    "results = recommend_articles(emotion=\"sad\")\n",
    "for article in results:\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 التاريخ: 05/07/2025\n",
      "✅ تم تحليل 5 منشور اليوم.\n",
      "\n",
      "📊 **نتائج التحليل**\n",
      "- عدد المنشورات الإيجابية: 0\n",
      "- عدد المنشورات السلبية: 0\n",
      "- عدد المنشورات المحايدة: 0\n",
      "\n",
      "\n",
      "✅ الخطة لليوم القادم:\n",
      "- تحليل بيانات إضافية.\n",
      "- تحسين دقة النموذج.\n",
      "- عرض النتائج بشكل رسومي مبسط.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import datetime\n",
    "\n",
    "def generate_daily_emotion_report(posts, model, vectorizer, label_encoder):\n",
    "    # تاريخ اليوم\n",
    "    today = datetime.date.today().strftime(\"%d/%m/%Y\")\n",
    "    \n",
    "    # تحويل النصوص إلى تمثيل عددي\n",
    "    X = vectorizer.transform(posts)\n",
    "    \n",
    "    # توقعات النموذج\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # تحويل التوقعات إلى labels مفهومة\n",
    "    labels = label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    # حساب عدد كل فئة\n",
    "    counter = Counter(labels)\n",
    "    \n",
    "    total_posts = len(posts)\n",
    "    positive_posts = [post for post, label in zip(posts, labels) if label == 'Positive']\n",
    "    negative_posts = [post for post, label in zip(posts, labels) if label == 'Negative']\n",
    "    neutral_posts  = [post for post, label in zip(posts, labels) if label == 'Neutral']\n",
    "    \n",
    "    print(f\"\\n📅 التاريخ: {today}\")\n",
    "    print(f\"✅ تم تحليل {total_posts} منشور اليوم.\\n\")\n",
    "    \n",
    "    print(\"📊 **نتائج التحليل**\")\n",
    "    print(f\"- عدد المنشورات الإيجابية: {counter.get('Positive', 0)}\")\n",
    "    print(f\"- عدد المنشورات السلبية: {counter.get('Negative', 0)}\")\n",
    "    print(f\"- عدد المنشورات المحايدة: {counter.get('Neutral', 0)}\\n\")\n",
    "    \n",
    "    if positive_posts:\n",
    "        print(\"🌱 أمثلة على منشورات إيجابية:\")\n",
    "        for p in positive_posts[:2]:  # مثال: أول 2\n",
    "            print(f\"  - {p}\")\n",
    "    \n",
    "    if negative_posts:\n",
    "        print(\"\\n⚠️ أمثلة على منشورات سلبية:\")\n",
    "        for p in negative_posts[:2]:\n",
    "            print(f\"  - {p}\")\n",
    "    \n",
    "    if neutral_posts:\n",
    "        print(\"\\nℹ️ أمثلة على منشورات محايدة:\")\n",
    "        for p in neutral_posts[:2]:\n",
    "            print(f\"  - {p}\")\n",
    "    \n",
    "    print(\"\\n✅ الخطة لليوم القادم:\")\n",
    "    print(\"- تحليل بيانات إضافية.\")\n",
    "    print(\"- تحسين دقة النموذج.\")\n",
    "    print(\"- عرض النتائج بشكل رسومي مبسط.\\n\")\n",
    "\n",
    "# ---------------------\n",
    "# مثال الاستخدام:\n",
    "daily_posts = [\n",
    "    \"I feel really anxious about school tomorrow.\",\n",
    "    \"Had fun with my friends today!\",\n",
    "    \"Why is everything going wrong in my life?\",\n",
    "    \"Watched a peaceful documentary about nature.\",\n",
    "    \"I feel so loved and appreciated.\"\n",
    "]\n",
    "\n",
    "generate_daily_emotion_report(\n",
    "    posts=daily_posts,\n",
    "    model=random_forest_model,\n",
    "    vectorizer=tfidf,\n",
    "    label_encoder=le\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
